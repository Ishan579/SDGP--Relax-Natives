[{"C:\\Users\\ASUS\\Desktop\\sem 2\\SDGP github code\\SDGP--Relax-Natives\\posenet\\src\\index.js":"1","C:\\Users\\ASUS\\Desktop\\sem 2\\SDGP github code\\SDGP--Relax-Natives\\posenet\\src\\App.js":"2","C:\\Users\\ASUS\\Desktop\\sem 2\\SDGP github code\\SDGP--Relax-Natives\\posenet\\src\\reportWebVitals.js":"3","C:\\Users\\ASUS\\Desktop\\sem 2\\SDGP github code\\SDGP--Relax-Natives\\posenet\\src\\utilities.js":"4"},{"size":500,"mtime":1614052150971,"results":"5","hashOfConfig":"6"},{"size":2502,"mtime":1614052150969,"results":"7","hashOfConfig":"6"},{"size":362,"mtime":1614052151020,"results":"8","hashOfConfig":"6"},{"size":6802,"mtime":1614052151022,"results":"9","hashOfConfig":"6"},{"filePath":"10","messages":"11","errorCount":0,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0},"942ulb",{"filePath":"12","messages":"13","errorCount":0,"warningCount":1,"fixableErrorCount":0,"fixableWarningCount":0,"source":"14"},{"filePath":"15","messages":"16","errorCount":0,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0},{"filePath":"17","messages":"18","errorCount":0,"warningCount":1,"fixableErrorCount":0,"fixableWarningCount":0,"source":"19"},"C:\\Users\\ASUS\\Desktop\\sem 2\\SDGP github code\\SDGP--Relax-Natives\\posenet\\src\\index.js",[],"C:\\Users\\ASUS\\Desktop\\sem 2\\SDGP github code\\SDGP--Relax-Natives\\posenet\\src\\App.js",["20"],"import React, {useRef} from 'react';\nimport './App.css';\nimport * as tf from \"@tensorflow/tfjs\";\nimport * as posenet from \"@tensorflow-models/posenet\";\nimport Webcam from \"react-webcam\";\nimport { drawKeypoints, drawSkeleton } from \"./utilities\";\n\n\nfunction App() {\n\n    const webcamRef = useRef(null);\n    const canvasRef = useRef(null);\n\n\n    //load posenet\n\n    const runPosenet = async () =>{\n      const net = await posenet.load({\n        inputResolution:{width:640, height:480},\n        scale:0.5\n      });\n\n      setInterval(()=>{\n        detect(net)\n      }, 100) //run \n    }\n\n    //Detect function.\n\n    const detect = async (net) => {\n      if(typeof webcamRef.current !== \"undefined\" && webcamRef.current !== null && \n      webcamRef.current.video.readyState===4) {\n        \n        //get video properties\n        const video = webcamRef.current.video;\n        const videoWidth = webcamRef.current.video.videoWidth;\n        const videoHeight = webcamRef.current.video.videoHeight;\n\n        //set video width\n        webcamRef.current.video.width = videoWidth;\n        webcamRef.current.video.height = videoHeight;\n\n        //Make detections\n        const pose = await net.estimateSinglePose(video);\n        console.log(pose);\n\n        //call draw function\n        drawCanvas(pose, video, videoWidth, videoHeight, canvasRef);\n\n      }\n    };\n\n\n    //Draw function\n    const drawCanvas = (pose, video, videoWidth, videoHeight, canvas) => {\n      const ctx = canvas.current.getContext(\"2d\");\n      canvas.current.width = videoWidth;\n      canvas.current.height = videoHeight;\n\n      drawKeypoints(pose[\"keypoints\"], 0.5, ctx);\n      drawSkeleton(pose[\"keypoints\"], 0.5, ctx);\n    };\n\n\n    runPosenet();\n\n  return (\n    <div className=\"App\">\n      <header className=\"App-header\">\n       <Webcam\n       ref={webcamRef}\n        style={{\n          position: \"absolute\",\n           marginLeft: \"auto\",\n            marginRight: \"auto\",\n            left: 0,\n            right: 0,\n            textAlign: \"center\",\n            zindex:9,\n            width:640,\n            height:480\n             }}\n             />\n\n\n            <canvas\n            ref={canvasRef}\n            style={{\n            position: \"absolute\",\n            marginLeft: \"auto\",\n            marginRight: \"auto\",\n            left: 0,\n            right: 0,\n            textAlign: \"center\",\n            zindex:9,\n            width:640,\n            height:480\n             }}/>\n\n      </header>\n    </div>\n  );\n}\n\nexport default App;\n","C:\\Users\\ASUS\\Desktop\\sem 2\\SDGP github code\\SDGP--Relax-Natives\\posenet\\src\\reportWebVitals.js",[],"C:\\Users\\ASUS\\Desktop\\sem 2\\SDGP github code\\SDGP--Relax-Natives\\posenet\\src\\utilities.js",["21"],"/**\n * @license\n * Copyright 2019 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * https://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport * as posenet from '@tensorflow-models/posenet';\nimport * as tf from '@tensorflow/tfjs-core';\n\nconst color = 'aqua';\nconst boundingBoxColor = 'red';\nconst lineWidth = 2;\n\nexport const tryResNetButtonName = 'tryResNetButton';\nexport const tryResNetButtonText = '[New] Try ResNet50';\nconst tryResNetButtonTextCss = 'width:100%;text-decoration:underline;';\nconst tryResNetButtonBackgroundCss = 'background:#e61d5f;';\n\nfunction isAndroid() {\n  return /Android/i.test(navigator.userAgent);\n}\n\nfunction isiOS() {\n  return /iPhone|iPad|iPod/i.test(navigator.userAgent);\n}\n\nexport function isMobile() {\n  return isAndroid() || isiOS();\n}\n\nfunction setDatGuiPropertyCss(propertyText, liCssString, spanCssString = '') {\n  var spans = document.getElementsByClassName('property-name');\n  for (var i = 0; i < spans.length; i++) {\n    var text = spans[i].textContent || spans[i].innerText;\n    if (text == propertyText) {\n      spans[i].parentNode.parentNode.style = liCssString;\n      if (spanCssString !== '') {\n        spans[i].style = spanCssString;\n      }\n    }\n  }\n}\n\nexport function updateTryResNetButtonDatGuiCss() {\n  setDatGuiPropertyCss(\n      tryResNetButtonText, tryResNetButtonBackgroundCss,\n      tryResNetButtonTextCss);\n}\n\n/**\n * Toggles between the loading UI and the main canvas UI.\n */\nexport function toggleLoadingUI(\n    showLoadingUI, loadingDivId = 'loading', mainDivId = 'main') {\n  if (showLoadingUI) {\n    document.getElementById(loadingDivId).style.display = 'block';\n    document.getElementById(mainDivId).style.display = 'none';\n  } else {\n    document.getElementById(loadingDivId).style.display = 'none';\n    document.getElementById(mainDivId).style.display = 'block';\n  }\n}\n\nfunction toTuple({y, x}) {\n  return [y, x];\n}\n\nexport function drawPoint(ctx, y, x, r, color) {\n  ctx.beginPath();\n  ctx.arc(x, y, r, 0, 2 * Math.PI);\n  ctx.fillStyle = color;\n  ctx.fill();\n}\n\n/**\n * Draws a line on a canvas, i.e. a joint\n */\nexport function drawSegment([ay, ax], [by, bx], color, scale, ctx) {\n  ctx.beginPath();\n  ctx.moveTo(ax * scale, ay * scale);\n  ctx.lineTo(bx * scale, by * scale);\n  ctx.lineWidth = lineWidth;\n  ctx.strokeStyle = color;\n  ctx.stroke();\n}\n\n/**\n * Draws a pose skeleton by looking up all adjacent keypoints/joints\n */\nexport function drawSkeleton(keypoints, minConfidence, ctx, scale = 1) {\n  const adjacentKeyPoints =\n      posenet.getAdjacentKeyPoints(keypoints, minConfidence);\n\n  adjacentKeyPoints.forEach((keypoints) => {\n    drawSegment(\n        toTuple(keypoints[0].position), toTuple(keypoints[1].position), color,\n        scale, ctx);\n  });\n}\n\n/**\n * Draw pose keypoints onto a canvas\n */\nexport function drawKeypoints(keypoints, minConfidence, ctx, scale = 1) {\n  for (let i = 0; i < keypoints.length; i++) {\n    const keypoint = keypoints[i];\n\n    if (keypoint.score < minConfidence) {\n      continue;\n    }\n\n    const {y, x} = keypoint.position;\n    drawPoint(ctx, y * scale, x * scale, 3, color);\n  }\n}\n\n/**\n * Draw the bounding box of a pose. For example, for a whole person standing\n * in an image, the bounding box will begin at the nose and extend to one of\n * ankles\n */\nexport function drawBoundingBox(keypoints, ctx) {\n  const boundingBox = posenet.getBoundingBox(keypoints);\n\n  ctx.rect(\n      boundingBox.minX, boundingBox.minY, boundingBox.maxX - boundingBox.minX,\n      boundingBox.maxY - boundingBox.minY);\n\n  ctx.strokeStyle = boundingBoxColor;\n  ctx.stroke();\n}\n\n/**\n * Converts an arary of pixel data into an ImageData object\n */\nexport async function renderToCanvas(a, ctx) {\n  const [height, width] = a.shape;\n  const imageData = new ImageData(width, height);\n\n  const data = await a.data();\n\n  for (let i = 0; i < height * width; ++i) {\n    const j = i * 4;\n    const k = i * 3;\n\n    imageData.data[j + 0] = data[k + 0];\n    imageData.data[j + 1] = data[k + 1];\n    imageData.data[j + 2] = data[k + 2];\n    imageData.data[j + 3] = 255;\n  }\n\n  ctx.putImageData(imageData, 0, 0);\n}\n\n/**\n * Draw an image on a canvas\n */\nexport function renderImageToCanvas(image, size, canvas) {\n  canvas.width = size[0];\n  canvas.height = size[1];\n  const ctx = canvas.getContext('2d');\n\n  ctx.drawImage(image, 0, 0);\n}\n\n/**\n * Draw heatmap values, one of the model outputs, on to the canvas\n * Read our blog post for a description of PoseNet's heatmap outputs\n * https://medium.com/tensorflow/real-time-human-pose-estimation-in-the-browser-with-tensorflow-js-7dd0bc881cd5\n */\nexport function drawHeatMapValues(heatMapValues, outputStride, canvas) {\n  const ctx = canvas.getContext('2d');\n  const radius = 5;\n  const scaledValues = heatMapValues.mul(tf.scalar(outputStride, 'int32'));\n\n  drawPoints(ctx, scaledValues, radius, color);\n}\n\n/**\n * Used by the drawHeatMapValues method to draw heatmap points on to\n * the canvas\n */\nfunction drawPoints(ctx, points, radius, color) {\n  const data = points.buffer().values;\n\n  for (let i = 0; i < data.length; i += 2) {\n    const pointY = data[i];\n    const pointX = data[i + 1];\n\n    if (pointX !== 0 && pointY !== 0) {\n      ctx.beginPath();\n      ctx.arc(pointX, pointY, radius, 0, 2 * Math.PI);\n      ctx.fillStyle = color;\n      ctx.fill();\n    }\n  }\n}\n\n/**\n * Draw offset vector values, one of the model outputs, on to the canvas\n * Read our blog post for a description of PoseNet's offset vector outputs\n * https://medium.com/tensorflow/real-time-human-pose-estimation-in-the-browser-with-tensorflow-js-7dd0bc881cd5\n */\n/*export function drawOffsetVectors(\n    heatMapValues, offsets, outputStride, scale = 1, ctx) {\n  const offsetPoints =\n      posenet.singlePose.getOffsetPoints(heatMapValues, outputStride, offsets);\n\n  const heatmapData = heatMapValues.buffer().values;\n  const offsetPointsData = offsetPoints.buffer().values;\n\n  for (let i = 0; i < heatmapData.length; i += 2) {\n    const heatmapY = heatmapData[i] * outputStride;\n    const heatmapX = heatmapData[i + 1] * outputStride;\n    const offsetPointY = offsetPointsData[i];\n    const offsetPointX = offsetPointsData[i + 1];\n\n    drawSegment(\n        [heatmapY, heatmapX], [offsetPointY, offsetPointX], color, scale, ctx);\n  }\n} */",{"ruleId":"22","severity":1,"message":"23","line":3,"column":13,"nodeType":"24","messageId":"25","endLine":3,"endColumn":15},{"ruleId":"26","severity":1,"message":"27","line":45,"column":14,"nodeType":"28","messageId":"29","endLine":45,"endColumn":16},"no-unused-vars","'tf' is defined but never used.","Identifier","unusedVar","eqeqeq","Expected '===' and instead saw '=='.","BinaryExpression","unexpected"]